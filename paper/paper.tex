\documentclass[a4paper]{article}
\usepackage{amsmath, bm}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\begin{document}
\begin{titlepage}
	\centering
	{\huge \bf Assignment 1\par}
	\vspace{1cm}
	{\Large Computational Intelligence, SS2018\par}
	\vspace{1cm}
	\begin{tabular}{|l|l|l|}
	\hline
	\multicolumn{3}{|c|}{\textbf{Team Members}}   \\ \hline
	Last name & First name & Matriculation Number \\ \hline
	Lee       & Eunseo     & 11739623             \\ \hline
	Shadley   & Alex       & 11739595             \\ \hline
	Lee       & Dayeong    & 11730321             \\ \hline
	\end{tabular}
\end{titlepage}

\section{Linear Regression}
\subsection{Derivation of Regularized Linear Regression}
asdfasdf

\subsection{Linear Regression with polynomial features}

The following plots demonstrate the results of Linear Regression with polynomial degrees of 1, 2, 5, and 20:

\noindent
\includegraphics[width=0.5\textwidth]{linreg_deg1.png}%
\includegraphics[width=0.5\textwidth]{linreg_deg2.png}\\[2em]
\includegraphics[width=0.5\textwidth]{linreg_deg5.png}%
\includegraphics[width=0.5\textwidth]{linreg_deg20.png}\par

\section{Logistic Regression}
\subsection{Derivation of Gradient}
\begin{equation}
\begin{split}
J(\bm{\theta}) &= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\bm{\theta}}(\bm{x}^{(i)}))
+ (1-y^{(i)} ) \log (1 - h_{\bm{\theta}}(\bm{x}^{(i)}))  \right) \\
&=  -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log (\sigma(\bm{x}^{(i)T}\bm{\theta}))
+ (1-y^{(i)} ) \log (1 - \sigma(\bm{x}^{(i)T}\bm{\theta})  \right)
\end{split}
\end{equation}
The partial derivative of the cost function with respect $\theta_j$ is
\begin{equation}
\begin{split}
\frac{\partial J(\bm{\theta})}{\partial \theta_j }
&= -\frac{1}{m} \sum_{i=1}^m \left(
y^{(i)} \frac{1}{\sigma(\bm{x}^{(i)T}\bm{\theta})} \frac{\partial \sigma(\bm{x}^{(i)T}\bm{\theta})}{\partial \theta_j}
- (1-y^{(i)}) \frac{1}{1-\sigma(\bm{x}^{(i)T}\bm{\theta})} \frac{\partial \sigma(\bm{x}^{(i)T}\bm{\theta})}{\partial \theta_j}
\right) \\
&= -\frac{1}{m} \sum_{i=1}^m \left(
y^{(i)} \frac{\sigma(\bm{x}^{(i)T}\bm{\theta}) \cdot (1 - \sigma(\bm{x}^{(i)T}\bm{\theta}))}{\sigma(\bm{x}^{(i)T}\bm{\theta})} \cdot \frac{\partial \bm{x}^{(i)T}\bm{\theta}}{\partial \theta_j}
 - (1 - y^{(i)}) \frac{\sigma(\bm{x}^{(i)T}\bm{\theta}) \cdot (1 - \sigma(\bm{x}^{(i)T}\bm{\theta}))}{1 - \sigma(\bm{x}^{(i)T}\bm{\theta})} \cdot \frac{\bm{x}^{(i)T}\bm{\theta}}{\partial \theta_j}
\right) \\
&= -\frac{1}{m} \sum_{i=1}^{m} \left(
y^{(i)} \frac{\sigma(\bm{x}^{(i)T}\bm{\theta}) \cdot (1 - \sigma(\bm{x}^{(i)T}\bm{\theta})) \cdot x_j^{(i)}}{\sigma(\bm{x}^{(i)T}\bm{\theta})}
- (1 - y^{(i)}) \frac{\sigma(\bm{x}^{(i)T}\bm{\theta}) \cdot (1 - \sigma(\bm{x}^{(i)T}\bm{\theta})) \cdot x_j^{(i)}}{1 - \sigma(\bm{x}^{(i)T}\bm{\theta})}
\right) \\
&= - \frac{1}{m} \sum_{i=1}^m \left(
y^{(i)} - y^{(i)} \sigma(\bm{x}^{(i)T}\bm{\theta}) + y^{(i)} \sigma(\bm{x}^{(i)T}\bm{\theta}) - \sigma(\bm{x}^{(i)T}\bm{\theta}) \right)\cdot x_j^{(i)} \\
&= -\frac{1}{m} \sum_{i=1}^m \left(
y^{(i)} - \sigma(\bm{x}^{(i)T}\bm{\theta})\right) \cdot x_j^{(i)} \\
&= \frac{1}{m} \sum_{i=1}^m \left( h_{\bm{\theta}}(\bm{x}^{(i)}) - y^{(i)}\right) \cdot x_j^{(i)}
\end{split}
\end{equation}
Thus, the gradient of the cost function is
\begin{equation}
\frac{\partial J(\bm{\theta})}{\partial \theta_j } = \frac{1}{m} \sum_{i=1}^m \left( h_{\bm{\theta}}(\bm{x}^{(i)}) - y^{(i)}\right) \cdot x_j^{(i)}
\end{equation}
\subsection{Logistic Regression training with gradient descent and scipy.optimize}
\subsubsection{Gradient descent}
\paragraph
1The function check\_gradient in toolbox.py is here to test if your gradient is well computed. Explain what it is doing
\paragraph
2The following plots demonstrate the results of Logistic Regression with degree 1 and learning rate $\eta$ = 1 for 20 and 2000 iterations :

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{logreg_deg1_iter20.png}
	\includegraphics[width=0.5\textwidth]{logreg_deg1_iter20_error.png}
	\caption{iteration = 20}
\end{figure}
\clearpage
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{logreg_deg1_iter2000.png}
	\includegraphics[width=0.5\textwidth]{logreg_deg1_iter2000_error.png}
	\caption{iteration = 2000}
\end{figure}

In case of figure 1, the process stops before it reaches the local minima. In case of figure 2, the process reaches the local minima in about 50 iterations. However, it continues to iterate even though the error does not change.  

When the number of iterations is too low, the logistic regression process finishes before reaching the local minima.When the number of iterations is too high, it takes a lot of time. Even though it reaches the local minima, it continues to iterate until it reaches the iteration number. Therefore, the number of iterations should not be too low and high.
\paragraph
3 The following plots demonstrate the results of Logistic Regression with degree = 2 , iterations = 200 and learning rate $\eta$ = \{.15, 1.5, 15.\} :
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta015.png}
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta015_error.png}
	\caption{learing rate $\eta$ = .15}
\end{figure}
\clearpage
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta105.png}
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta105_error.png}
	\caption{learing rate $\eta$ = 1.5}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta15.png}
	\includegraphics[width=0.5\textwidth]{logreg_deg2_iter200_eta15_error.png}
	\caption{learing rate $\eta$ = 15}
\end{figure}

In case of figure 3, the process stops before it reaches the local minima. In case of figure 4, the process reaches the local minima. In case of figure 5, it doesn't reach the local minima by overshooting the local minima.

When learning rate is too low, it takes a lot of time to reach the local minima or it ends before reaching the local minima. When learning rate is too big, it can overshoot the local minima so it may not reach the local minima or may even diverge.

\paragraph
4
\paragraph
5
\end{document}
